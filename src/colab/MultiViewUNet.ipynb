{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown \"1QAKYbMwhP4ITGDFfR3fVFWIxfp4uojbJ\"\n",
    "!jar - xvf \"Dataset_v1.zip\" > /dev/null\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm\n",
    "from numpy.typing import NDArray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH      = \"Dataset/Input\"\n",
    "TARGET_PATH     = \"Dataset/Target\"\n",
    "IMG_SIZE        = 256\n",
    "BATCH_SIZE      = 4\n",
    "VAL_SPLIT       = 0.2\n",
    "LEARNING_RATE   = 3e-4\n",
    "N_EPOCHS        = 300\n",
    "PATIENCE        = 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet:\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int,\n",
    "        n_channels: int = 3,\n",
    "        width: int = 32,\n",
    "        depth: int = 4,\n",
    "        kernel_size: int = 3\n",
    "    ):\n",
    "        self.img_size = img_size\n",
    "        self.n_channels = n_channels\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    @staticmethod\n",
    "    def conv(\n",
    "        x: tf.Tensor,\n",
    "        filters: int,\n",
    "        kernel_size: int\n",
    "    ) -> tf.Tensor:\n",
    "        for i in range(2):\n",
    "            x = tf.keras.layers.Conv2D(\n",
    "                filters=filters,\n",
    "                kernel_size=kernel_size,\n",
    "                strides=1,\n",
    "                padding=\"same\",\n",
    "                data_format=\"channels_last\",\n",
    "                dilation_rate=1,\n",
    "                groups=1,\n",
    "                activation=None,\n",
    "                use_bias=True,\n",
    "                kernel_initializer=\"glorot_uniform\",\n",
    "                bias_initializer=\"zeros\"\n",
    "            )(x)\n",
    "\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "            x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def deconv(x: tf.Tensor, filters: int) -> tf.Tensor:\n",
    "        x = tf.keras.layers.Conv2DTranspose(\n",
    "            filters=filters,\n",
    "            kernel_size=2,\n",
    "            strides=2,\n",
    "            padding=\"same\",\n",
    "            output_padding=None,\n",
    "            data_format=None,\n",
    "            dilation_rate=1,\n",
    "            activation=None,\n",
    "            use_bias=True,\n",
    "            kernel_initializer=\"glorot_uniform\",\n",
    "            bias_initializer=\"zeros\"\n",
    "        )(x)\n",
    "\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def output(x: tf.Tensor) -> tf.Tensor:\n",
    "        return tf.keras.layers.Conv2D(3, (1, 1), activation=\"sigmoid\")(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def pool(x: tf.Tensor, pool_size: int = 2) -> tf.Tensor:\n",
    "        return tf.keras.layers.MaxPool2D(pool_size)(x)\n",
    "\n",
    "    def __call__(self) -> tf.keras.Model:\n",
    "        inputs = tf.keras.layers.Input(\n",
    "            shape=(self.img_size, self.img_size, self.n_channels)\n",
    "        )\n",
    "\n",
    "        # scaled = tf.keras.layers.Rescaling(1./255.0, offset=0)(inputs)\n",
    "\n",
    "        # ------------------ Downsampling ---------------------\n",
    "        downsample_layers = []\n",
    "        downsample_layers.append(\n",
    "            self.conv(\n",
    "                x=inputs,\n",
    "                filters=self.width,\n",
    "                kernel_size=self.kernel_size\n",
    "            )\n",
    "        )\n",
    "        for i in range(1, self.depth):\n",
    "            filters = int((2 ** i) * self.width)\n",
    "            downsample_layers.append(\n",
    "                self.pool(\n",
    "                    self.conv(\n",
    "                        x=downsample_layers[i - 1],\n",
    "                        filters=filters,\n",
    "                        kernel_size=self.kernel_size\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # ------------------- Features --------------------\n",
    "        n_features = int((2 ** self.depth) * self.width)\n",
    "        self.features = self.pool(\n",
    "            self.conv(\n",
    "                x=downsample_layers[-1],\n",
    "                filters=n_features,\n",
    "                kernel_size=self.kernel_size\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # ------------------- Upsampling --------------------\n",
    "        upsample_layers = []\n",
    "        upsample_layers.append(self.features)\n",
    "        for i in range(1, self.depth + 1):\n",
    "            filters = int((2 ** (self.depth - i)) * self.width)\n",
    "            upsample_layers.append(\n",
    "                self.conv(\n",
    "                    x=tf.keras.layers.concatenate([\n",
    "                        downsample_layers[self.depth - i],\n",
    "                        self.deconv(\n",
    "                            x=upsample_layers[i - 1],\n",
    "                            filters=filters\n",
    "                        )\n",
    "                    ]),\n",
    "                    filters=filters,\n",
    "                    kernel_size=self.kernel_size\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # ---------------------- Output -----------------------\n",
    "        outputs = self.output(upsample_layers[-1])\n",
    "\n",
    "        return tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_mse(y_true, y_pred):\n",
    "    _y_true = y_true[y_true != 1.0]\n",
    "    _y_pred = y_pred[y_true != 1.0]\n",
    "    squared_difference = tf.square(_y_true - _y_pred)\n",
    "    return tf.reduce_mean(squared_difference, axis=-1)\n",
    "\n",
    "\n",
    "def attention_mae(y_true, y_pred):\n",
    "    _y_true = y_true[y_true != 1.0]\n",
    "    _y_pred = y_pred[y_true != 1.0]\n",
    "    squared_difference = tf.abs(_y_true - _y_pred)\n",
    "    return tf.reduce_mean(squared_difference, axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_dir(path: str, subset: str) -> tf.data.Dataset:\n",
    "    return tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=path,\n",
    "        labels=None,\n",
    "        color_mode='rgb',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        image_size=(IMG_SIZE, IMG_SIZE),\n",
    "        shuffle=True,\n",
    "        seed=42,\n",
    "        validation_split=VAL_SPLIT,\n",
    "        subset=subset,\n",
    "        interpolation='bilinear',\n",
    "        follow_links=False,\n",
    "        crop_to_aspect_ratio=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = load_data_from_dir(\"Dataset/Input\", \"training\")\n",
    "trainY = load_data_from_dir(\"Dataset/Target\", \"training\")\n",
    "testX = load_data_from_dir(\"Dataset/Input\", \"validation\")\n",
    "testY = load_data_from_dir(\"Dataset/Target\", \"validation\")\n",
    "\n",
    "train_ds = tf.data.Dataset.zip(trainX, trainY)\n",
    "test_ds = tf.data.Dataset.zip(testX, testY)\n",
    "\n",
    "print(train_ds.element_spec)\n",
    "print(test_ds.element_spec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=PATIENCE\n",
    "    )\n",
    "]\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=LEARNING_RATE\n",
    ")\n",
    "\n",
    "model = UNet(IMG_SIZE)()\n",
    "\n",
    "model.compile(\n",
    "    loss=attention_mse,\n",
    "    optimizer=optimizer,\n",
    "    metrics=['mean_squared_error']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=N_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('Ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc12dd51c08fff59e312c49b5273cbf2a12939660509ea1a4d83cd89b0463726"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
