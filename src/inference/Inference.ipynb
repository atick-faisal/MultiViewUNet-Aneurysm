{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atick-faisal/TAVI/blob/main/src/inference/Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTm2jAubIjRQ"
      },
      "source": [
        "# Fix for GDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0Ewr7w7KIjRT"
      },
      "outputs": [],
      "source": [
        "!pip install -U --no-cache-dir gdown --pre > /dev/null\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7_3Gl6DIjRU"
      },
      "source": [
        "# Mount GDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1ufvvLvTIjRU",
        "outputId": "ece794bc-88d2-4a5c-8070-5371b0669451",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR16NOU-IjRV"
      },
      "source": [
        "# Download and Extract Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8jtCFRNXIjRV",
        "outputId": "245eb2ae-4003-4b47-d5f2-dfb615240a77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (uriginal): https://drive.google.com/uc?id=1Hkn-xh9gqjWOCkFA0OwzvWPDRsIPwdSv\n",
            "From (redirected): https://drive.google.com/uc?id=1Hkn-xh9gqjWOCkFA0OwzvWPDRsIPwdSv&confirm=t&uuid=f3ad0633-aee7-4ac1-84da-c2385d2e9f65\n",
            "To: /content/AAA_DATASET_v7.zip\n",
            "100% 3.42G/3.42G [00:44<00:00, 76.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown \"1Hkn-xh9gqjWOCkFA0OwzvWPDRsIPwdSv\"\n",
        "!unzip -o \"AAA_DATASET_v7.zip\" > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIlInt82IjRV"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "WulcMmiGIjRV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "import matplotlib\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# matplotlib.use('Agg')\n",
        "plt.rcParams[\"font.size\"] = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S13GCOm3IjRW"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "7YDdAI5sIjRW"
      },
      "outputs": [],
      "source": [
        "PROBLEM = \"Curvature_2_ECAP\"\n",
        "\n",
        "MODEL_NAME = \"MultiViewUNet_Curvature_2_TAWSS_2_ECAP\"\n",
        "DATASET_DIR = \"/content/Images/\"\n",
        "TRAIN_DIR = \"Train/\"\n",
        "TEST_DIR = \"Test/\"\n",
        "INPUT_DIR = PROBLEM.split(\"_2_\")[0]\n",
        "TARGET_DIR = PROBLEM.split(\"_2_\")[1]\n",
        "MODEL_DIR = \"/content/drive/MyDrive/Research/TAVI/Models/\"\n",
        "PRED_DIR = \"/content/drive/MyDrive/Research/TAVI/Predictions/\"\n",
        "IMG_SIZE = 256\n",
        "BATCH_SIZE = 16\n",
        "BUFFER_SIZE = 1000\n",
        "VAL_SPLIT = 0.2\n",
        "LEARNING_RATE = 0.001\n",
        "N_EPOCHS = 300\n",
        "PATIENCE = 30\n",
        "\n",
        "MODEL_PATH = f\"{MODEL_DIR}/Curvature_2_TAWSS_MultiViewUNet_I256_B16_LR0.001/Apr-04-01:12AM\"\n",
        "SECOND_MODEL_PATH = f\"{MODEL_DIR}/TAWSS_2_ECAP_MultiViewUNet_I256_B16_LR0.001/Apr-03-03:09PM\"\n",
        "\n",
        "EXP_NAME = f\"{MODEL_NAME}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XSi93DbIjRX"
      },
      "source": [
        "# DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "hCPzeEngIjRX"
      },
      "outputs": [],
      "source": [
        "def load_data_from_dir(path: str) -> tf.data.Dataset:\n",
        "    return tf.keras.utils.image_dataset_from_directory(\n",
        "        directory=path,\n",
        "        labels=None,\n",
        "        color_mode='rgb',\n",
        "        batch_size=BATCH_SIZE,\n",
        "        image_size=(IMG_SIZE, IMG_SIZE),\n",
        "        shuffle=False,\n",
        "        seed=42,\n",
        "        interpolation='bilinear',\n",
        "        follow_links=False,\n",
        "        crop_to_aspect_ratio=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVXnzYHQIjRX"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ny6INDiUIjRX",
        "outputId": "75964d51-6b41-4500-8b90-4dc7d26ee56c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8208 files belonging to 1 classes.\n",
            "Found 8208 files belonging to 1 classes.\n",
            "Found 312 files belonging to 1 classes.\n",
            "Found 312 files belonging to 1 classes.\n",
            "(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None))\n",
            "(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None))\n"
          ]
        }
      ],
      "source": [
        "trainX = load_data_from_dir(os.path.join(DATASET_DIR, TRAIN_DIR, INPUT_DIR))\n",
        "trainY = load_data_from_dir(os.path.join(DATASET_DIR, TRAIN_DIR, TARGET_DIR))\n",
        "testX = load_data_from_dir(os.path.join(DATASET_DIR, TEST_DIR, INPUT_DIR))\n",
        "testY = load_data_from_dir(os.path.join(DATASET_DIR, TEST_DIR, TARGET_DIR))\n",
        "\n",
        "train_ds = tf.data.Dataset.zip((trainX, trainY))\n",
        "test_ds = tf.data.Dataset.zip((testX, testY))\n",
        "\n",
        "print(train_ds.element_spec)\n",
        "print(test_ds.element_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGxowp6vIjRX"
      },
      "source": [
        "# Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Qs0iTc0-IjRY"
      },
      "outputs": [],
      "source": [
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "train_ds = train_ds.map(lambda x, y: (\n",
        "    normalization_layer(x), normalization_layer(y)))\n",
        "test_ds = test_ds.map(lambda x, y: (\n",
        "    normalization_layer(x), normalization_layer(y)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uhC7BbbIjRY"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "6Sp-k9N5IjRY"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_batches = (\n",
        "    train_ds\n",
        "    .cache()\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .prefetch(buffer_size=AUTOTUNE)\n",
        ")\n",
        "\n",
        "test_batches = (\n",
        "    test_ds\n",
        "    .cache()\n",
        "    .prefetch(buffer_size=AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji2VPUSlIjRY"
      },
      "source": [
        "# Loss Functions / Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "RJ7EmwkTIjRY"
      },
      "outputs": [],
      "source": [
        "def attention_mse(y_true, y_pred):\n",
        "    _y_true = y_true[y_true != 1.0]\n",
        "    _y_pred = y_pred[y_true != 1.0]\n",
        "    squared_difference = tf.square(_y_true - _y_pred)\n",
        "    return tf.reduce_mean(squared_difference, axis=-1)\n",
        "\n",
        "\n",
        "def attention_mae(y_true, y_pred):\n",
        "    _y_true = y_true[y_true != 1.0]\n",
        "    _y_pred = y_pred[y_true != 1.0]\n",
        "    squared_difference = tf.abs(_y_true - _y_pred)\n",
        "    return tf.reduce_mean(squared_difference, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scpyp787IjRY"
      },
      "source": [
        "# Load Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "UeTuZrzqIjRZ"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model(\n",
        "    MODEL_PATH,\n",
        "    custom_objects={\n",
        "        \"attention_mse\": attention_mse,\n",
        "        \"attention_mae\": attention_mae\n",
        "    }\n",
        ")\n",
        "second_model = tf.keras.models.load_model(\n",
        "    SECOND_MODEL_PATH,\n",
        "    custom_objects={\n",
        "        \"attention_mse\": attention_mse,\n",
        "        \"attention_mae\": attention_mae\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUDC5rvVIjRZ"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timestamp = datetime.datetime.now().strftime('%b-%d-%I:%M%p')\n"
      ],
      "metadata": {
        "id": "nD7lP6kKq3h1"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_path = os.path.join(PRED_DIR, EXP_NAME, timestamp)\n",
        "if not os.path.exists(pred_path):\n",
        "    os.makedirs(pred_path)\n",
        "\n",
        "emae, emse = 0.0, 0.0\n",
        "\n",
        "for idx, (input, target) in enumerate(test_batches.unbatch()):\n",
        "    prediction = tf.squeeze(\n",
        "        model.predict(\n",
        "                tf.expand_dims(input, axis=0),\n",
        "                verbose=0\n",
        "            )\n",
        "        ).numpy()\n",
        "    \n",
        "    prediction[\n",
        "        tf.math.reduce_all((target == [1.0, 1.0, 1.0]), axis=-1)\n",
        "    ] = [1.0, 1.0, 1.0]\n",
        "\n",
        "    second_prediction = tf.squeeze(\n",
        "        second_model.predict(\n",
        "                tf.expand_dims(prediction, axis=0),\n",
        "                verbose=0\n",
        "            )\n",
        "        ).numpy()\n",
        "\n",
        "    second_prediction[\n",
        "        tf.math.reduce_all((target == [1.0, 1.0, 1.0]), axis=-1)\n",
        "    ] = [1.0, 1.0, 1.0]\n",
        "\n",
        "    loss, acc = second_model.evaluate(\n",
        "        tf.expand_dims(prediction, axis=0),\n",
        "        tf.expand_dims(target, axis=0)\n",
        "    )\n",
        "\n",
        "    emae = emae + acc\n",
        "    emse = emse + loss\n",
        "\n",
        "    plt.figure(figsize=(7, 7))\n",
        "    plt.imshow(target)\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(pred_path, f\"{idx}_T.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure(figsize=(7, 7))\n",
        "    plt.imshow(second_prediction)\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(pred_path, f\"{idx}_P.png\"))\n",
        "    plt.close()\n",
        "\n",
        "print(f\"EMAE: {emae / (idx + 1)}\")\n",
        "print(f\"EMSE: {emse / (idx + 1)}\")"
      ],
      "metadata": {
        "id": "63DJVJ_7lmHb",
        "outputId": "e87aff66-acf7-44ff-d4dd-9d5feb33eb91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 896ms/step - loss: 0.0517 - attention_mae: 0.1320\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0436 - attention_mae: 0.1182\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0654 - attention_mae: 0.1584\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0805 - attention_mae: 0.1744\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0872 - attention_mae: 0.1854\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0807 - attention_mae: 0.1765\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0538 - attention_mae: 0.1411\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0447 - attention_mae: 0.1291\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0378 - attention_mae: 0.1179\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0432 - attention_mae: 0.1296\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0577 - attention_mae: 0.1507\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0585 - attention_mae: 0.1465\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0552 - attention_mae: 0.1521\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0444 - attention_mae: 0.1306\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0446 - attention_mae: 0.1319\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0551 - attention_mae: 0.1516\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0304 - attention_mae: 0.1010\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0266 - attention_mae: 0.1032\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0416 - attention_mae: 0.1299\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0531 - attention_mae: 0.1455\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0442 - attention_mae: 0.1286\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0451 - attention_mae: 0.1397\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0370 - attention_mae: 0.1306\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0323 - attention_mae: 0.1146\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0773 - attention_mae: 0.1815\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0937 - attention_mae: 0.1936\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0914 - attention_mae: 0.1956\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0637 - attention_mae: 0.1703\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0872 - attention_mae: 0.1954\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0818 - attention_mae: 0.1944\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1169 - attention_mae: 0.2339\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0657 - attention_mae: 0.1641\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0350 - attention_mae: 0.1083\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0398 - attention_mae: 0.1213\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0325 - attention_mae: 0.1199\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0466 - attention_mae: 0.1266\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0305 - attention_mae: 0.1091\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0292 - attention_mae: 0.0986\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0356 - attention_mae: 0.1047\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0350 - attention_mae: 0.1061\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0375 - attention_mae: 0.1089\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0575 - attention_mae: 0.1400\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0512 - attention_mae: 0.1353\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0453 - attention_mae: 0.1304\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0539 - attention_mae: 0.1486\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0692 - attention_mae: 0.1774\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0465 - attention_mae: 0.1475\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0312 - attention_mae: 0.1074\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0351 - attention_mae: 0.1151\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0442 - attention_mae: 0.1303\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0449 - attention_mae: 0.1278\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0278 - attention_mae: 0.0985\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0233 - attention_mae: 0.0895\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0382 - attention_mae: 0.1153\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0539 - attention_mae: 0.1395\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0422 - attention_mae: 0.1239\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0196 - attention_mae: 0.0805\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0236 - attention_mae: 0.0868\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0264 - attention_mae: 0.0918\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0278 - attention_mae: 0.0973\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0228 - attention_mae: 0.0872\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.0260 - attention_mae: 0.0940\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.0392 - attention_mae: 0.1183\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0350 - attention_mae: 0.1166\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0307 - attention_mae: 0.1095\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0605 - attention_mae: 0.1518\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0611 - attention_mae: 0.1522\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0368 - attention_mae: 0.1159\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0169 - attention_mae: 0.0758\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0147 - attention_mae: 0.0715\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0200 - attention_mae: 0.0814\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0169 - attention_mae: 0.0756\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0748 - attention_mae: 0.1628\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0686 - attention_mae: 0.1560\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0420 - attention_mae: 0.1161\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0376 - attention_mae: 0.1102\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0331 - attention_mae: 0.1067\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.1018 - attention_mae: 0.1993\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0589 - attention_mae: 0.1450\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.0629 - attention_mae: 0.1526\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0778 - attention_mae: 0.1706\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0501 - attention_mae: 0.1286\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0724 - attention_mae: 0.1593\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0748 - attention_mae: 0.1682\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0475 - attention_mae: 0.1443\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0646 - attention_mae: 0.1646\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0671 - attention_mae: 0.1634\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0698 - attention_mae: 0.1634\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0634 - attention_mae: 0.1498\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0594 - attention_mae: 0.1497\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0480 - attention_mae: 0.1300\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0393 - attention_mae: 0.1209\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0342 - attention_mae: 0.1144\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0396 - attention_mae: 0.1335\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0375 - attention_mae: 0.1287\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0432 - attention_mae: 0.1360\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0103 - attention_mae: 0.0594\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0124 - attention_mae: 0.0685\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0208 - attention_mae: 0.0930\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0323 - attention_mae: 0.1135\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0553 - attention_mae: 0.1459\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0547 - attention_mae: 0.1394\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0303 - attention_mae: 0.1025\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0173 - attention_mae: 0.0721\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0145 - attention_mae: 0.0682\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0125 - attention_mae: 0.0636\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0133 - attention_mae: 0.0665\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0150 - attention_mae: 0.0702\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0373 - attention_mae: 0.1138\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0335 - attention_mae: 0.1173\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0347 - attention_mae: 0.1227\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0371 - attention_mae: 0.1247\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0597 - attention_mae: 0.1589\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0607 - attention_mae: 0.1588\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0342 - attention_mae: 0.1148\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0199 - attention_mae: 0.0846\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.0183 - attention_mae: 0.0804\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0168 - attention_mae: 0.0740\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.0227 - attention_mae: 0.0844\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0306 - attention_mae: 0.1008\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0620 - attention_mae: 0.1503\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0636 - attention_mae: 0.1617\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0706 - attention_mae: 0.1778\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0855 - attention_mae: 0.2055\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0931 - attention_mae: 0.2077\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0690 - attention_mae: 0.1712\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0435 - attention_mae: 0.1352\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0298 - attention_mae: 0.1036\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0235 - attention_mae: 0.0913\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0241 - attention_mae: 0.0912\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0261 - attention_mae: 0.0967\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0291 - attention_mae: 0.0982\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0922 - attention_mae: 0.2031\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1074 - attention_mae: 0.2244\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1101 - attention_mae: 0.2347\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.1684 - attention_mae: 0.3069\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.1031 - attention_mae: 0.2181\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0735 - attention_mae: 0.1795\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0705 - attention_mae: 0.1799\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0562 - attention_mae: 0.1506\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0563 - attention_mae: 0.1590\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0974 - attention_mae: 0.2241\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0843 - attention_mae: 0.1977\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0918 - attention_mae: 0.2056\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0802 - attention_mae: 0.1668\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0675 - attention_mae: 0.1461\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0716 - attention_mae: 0.1531\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0780 - attention_mae: 0.1669\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0669 - attention_mae: 0.1522\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0732 - attention_mae: 0.1622\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0399 - attention_mae: 0.1174\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0333 - attention_mae: 0.1057\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0332 - attention_mae: 0.1102\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0493 - attention_mae: 0.1295\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.0485 - attention_mae: 0.1322\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0657 - attention_mae: 0.1576\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0425 - attention_mae: 0.1242\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0455 - attention_mae: 0.1360\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0420 - attention_mae: 0.1283\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0429 - attention_mae: 0.1273\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0477 - attention_mae: 0.1390\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0505 - attention_mae: 0.1433\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0623 - attention_mae: 0.1590\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0460 - attention_mae: 0.1351\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0483 - attention_mae: 0.1428\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0495 - attention_mae: 0.1418\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0448 - attention_mae: 0.1318\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0462 - attention_mae: 0.1259\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0079 - attention_mae: 0.0544\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0079 - attention_mae: 0.0540\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0083 - attention_mae: 0.0576\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0087 - attention_mae: 0.0594\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0118 - attention_mae: 0.0638\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0140 - attention_mae: 0.0758\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0134 - attention_mae: 0.0723\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0098 - attention_mae: 0.0634\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0074 - attention_mae: 0.0561\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0082 - attention_mae: 0.0596\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0104 - attention_mae: 0.0633\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0098 - attention_mae: 0.0603\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0329 - attention_mae: 0.1026\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0440 - attention_mae: 0.1223\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0582 - attention_mae: 0.1521\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0579 - attention_mae: 0.1492\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1175 - attention_mae: 0.2349\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.1312 - attention_mae: 0.2471\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0489 - attention_mae: 0.1332\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0431 - attention_mae: 0.1250\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0442 - attention_mae: 0.1239\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0512 - attention_mae: 0.1375\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0669 - attention_mae: 0.1596\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0532 - attention_mae: 0.1392\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1190 - attention_mae: 0.2263\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1303 - attention_mae: 0.2453\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0846 - attention_mae: 0.1962\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0852 - attention_mae: 0.1872\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0577 - attention_mae: 0.1420\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0590 - attention_mae: 0.1441\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0521 - attention_mae: 0.1415\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0623 - attention_mae: 0.1514\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0481 - attention_mae: 0.1363\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0397 - attention_mae: 0.1222\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0578 - attention_mae: 0.1442\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0968 - attention_mae: 0.1947\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0891 - attention_mae: 0.1994\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.1215 - attention_mae: 0.2449\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1437 - attention_mae: 0.2720\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.1497 - attention_mae: 0.2815\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.1615 - attention_mae: 0.2962\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1473 - attention_mae: 0.2746\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.1062 - attention_mae: 0.2232\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0798 - attention_mae: 0.1827\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0678 - attention_mae: 0.1671\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0651 - attention_mae: 0.1639\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0595 - attention_mae: 0.1424\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0723 - attention_mae: 0.1658\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0922 - attention_mae: 0.1954\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0896 - attention_mae: 0.1934\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0780 - attention_mae: 0.1812\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0660 - attention_mae: 0.1549\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0834 - attention_mae: 0.1884\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0739 - attention_mae: 0.1667\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0675 - attention_mae: 0.1622\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0554 - attention_mae: 0.1433\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0445 - attention_mae: 0.1286\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0433 - attention_mae: 0.1207\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0706 - attention_mae: 0.1630\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0678 - attention_mae: 0.1676\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0167 - attention_mae: 0.0675\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0123 - attention_mae: 0.0628\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0261 - attention_mae: 0.0917\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0239 - attention_mae: 0.0887\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0286 - attention_mae: 0.1001\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0332 - attention_mae: 0.1114\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0357 - attention_mae: 0.1188\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.0363 - attention_mae: 0.1211\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0249 - attention_mae: 0.0969\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0136 - attention_mae: 0.0709\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0131 - attention_mae: 0.0659\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0127 - attention_mae: 0.0633\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0335 - attention_mae: 0.1075\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0415 - attention_mae: 0.1171\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0293 - attention_mae: 0.1065\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0251 - attention_mae: 0.1011\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0334 - attention_mae: 0.1068\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0233 - attention_mae: 0.0846\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0178 - attention_mae: 0.0779\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0177 - attention_mae: 0.0738\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0218 - attention_mae: 0.0856\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0199 - attention_mae: 0.0812\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0182 - attention_mae: 0.0786\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0209 - attention_mae: 0.0835\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0865 - attention_mae: 0.1856\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1107 - attention_mae: 0.2101\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.1256 - attention_mae: 0.2295\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.1011 - attention_mae: 0.2042\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0907 - attention_mae: 0.1876\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0638 - attention_mae: 0.1543\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0549 - attention_mae: 0.1372\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0433 - attention_mae: 0.1290\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0374 - attention_mae: 0.1206\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0328 - attention_mae: 0.1081\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0427 - attention_mae: 0.1244\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0418 - attention_mae: 0.1242\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0443 - attention_mae: 0.1269\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0717 - attention_mae: 0.1641\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0732 - attention_mae: 0.1642\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0962 - attention_mae: 0.2025\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0942 - attention_mae: 0.1934\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.1081 - attention_mae: 0.2032\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0800 - attention_mae: 0.1701\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0578 - attention_mae: 0.1397\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0479 - attention_mae: 0.1343\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0508 - attention_mae: 0.1379\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0492 - attention_mae: 0.1354\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0597 - attention_mae: 0.1531\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0282 - attention_mae: 0.0996\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0438 - attention_mae: 0.1249\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0575 - attention_mae: 0.1441\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0473 - attention_mae: 0.1345\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0369 - attention_mae: 0.1202\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0281 - attention_mae: 0.1079\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0460 - attention_mae: 0.1263\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0527 - attention_mae: 0.1440\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0615 - attention_mae: 0.1541\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0428 - attention_mae: 0.1206\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0314 - attention_mae: 0.1041\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.0238 - attention_mae: 0.0910\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.0141 - attention_mae: 0.0642\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0140 - attention_mae: 0.0634\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0180 - attention_mae: 0.0721\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0116 - attention_mae: 0.0593\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.0180 - attention_mae: 0.0716\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0178 - attention_mae: 0.0714\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0177 - attention_mae: 0.0720\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0235 - attention_mae: 0.0803\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.0322 - attention_mae: 0.0937\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0253 - attention_mae: 0.0844\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.0388 - attention_mae: 0.1010\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0240 - attention_mae: 0.0778\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0330 - attention_mae: 0.1011\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0376 - attention_mae: 0.1088\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0328 - attention_mae: 0.1042\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0485 - attention_mae: 0.1348\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0451 - attention_mae: 0.1235\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0507 - attention_mae: 0.1299\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0568 - attention_mae: 0.1377\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0414 - attention_mae: 0.1200\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0364 - attention_mae: 0.1102\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0387 - attention_mae: 0.1151\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0383 - attention_mae: 0.1174\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.0391 - attention_mae: 0.1152\n",
            "EMAE: 0.13379853134019634\n",
            "EMSE: 0.05056985643787835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "udjFU9DRbB1R"
      },
      "execution_count": 49,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}